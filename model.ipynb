{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"prot_seq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 16567\n",
      "Number of rows after filtering: 16567\n",
      "\n",
      "First few rows of filtered data:\n",
      "   index       value                                             result\n",
      "0      0  A0A024R2I8  MTPNSMTENGLTAWDKPKHCPDREHDWKLVGMSEACLHRKSHSERR...\n",
      "1      1  A0A075B5G3  GVSDVPRDLEVVAATPTSLLISWPPPSHGYGYYRITYGETGGNSPV...\n",
      "2      2  A0A075B6N1  MSNQVLCCVVLCLLGANTVDGGITQSPKYLFRKEGQNVTLSCEQNL...\n",
      "3      3  A0A075B6T6  MKSLRVLLVILWLQLSWVWSQQKEVEQNSGPLSVPEGAIASLNCTY...\n",
      "4      4  A0A087WZ82  MELVLVFLCSLLAPMVLASAAEKEKEMDPFHYDYQTLRIGGLVFAV...\n"
     ]
    }
   ],
   "source": [
    "# Using str.contains() with ~ operator to negate\n",
    "df = df[~df[\"result\"].str.contains(\"Error\", na=False)].reset_index()\n",
    "# protein_sequences = df[df[\"result\"].str.len() < 10000]\n",
    "protein_sequences = df\n",
    "# Display basic information\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "print(f\"Number of rows after filtering: {len(protein_sequences)}\")\n",
    "print(\"\\nFirst few rows of filtered data:\")\n",
    "print(protein_sequences.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = list(df['result'])\n",
    "# vals = [(len(x),i) for i,x in enumerate(vals)]\n",
    "# print(sorted(vals)[-10:])\n",
    "remove_indices = [12668, 12667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "Embedding Matrix Shape: torch.Size([3905, 1280])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Load ESM-2 model\n",
    "esm_model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(esm_model_name)\n",
    "model = EsmModel.from_pretrained(esm_model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Function to extract embeddings\n",
    "def get_protein_embedding(sequence):\n",
    "    inputs = tokenizer(sequence, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    torch.cuda.empty_cache()\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "\n",
    "protein_embeddings = {}\n",
    "for index, row in protein_sequences.iterrows():\n",
    "    if index < 12660 or index in remove_indices:\n",
    "        continue\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    protein_id, sequence = row[\"value\"], row[\"result\"]\n",
    "    protein_embeddings[protein_id] = get_protein_embedding(sequence)\n",
    "\n",
    "# Convert to tensor matrix\n",
    "embedding_dim = list(protein_embeddings.values())[0].shape[0]\n",
    "protein_embedding_matrix = torch.stack(list(protein_embeddings.values()))\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", protein_embedding_matrix.shape)  # Expected: (num_proteins, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_836052/1296621124.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_embeddings = torch.load(\"protein_embeddings1.pt\")\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings = torch.load(\"protein_embeddings1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_embeddings.update(protein_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(loaded_embeddings, 'protein_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = protein_embeddings\n",
    "protein_embeddings = loaded_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([16565, 1280])\n"
     ]
    }
   ],
   "source": [
    "# Ensure a consistent protein ID ordering\n",
    "sorted_protein_ids = sorted(protein_embeddings.keys())  # Sort alphabetically or numerically\n",
    "\n",
    "# Convert the embeddings dictionary into an ordered list\n",
    "X_train = torch.stack([protein_embeddings[pid] for pid in sorted_protein_ids])\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # Should be (num_proteins, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv('HomoSapiens_binary_hq.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique proteins from Uniprot_A: 16565\n",
      "Shape of adjacency matrix: (16565, 16565)\n",
      "Number of non-zero elements: 290497\n",
      "Sparsity: 0.11%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16565, 16565])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "proteins = sorted_protein_ids\n",
    "# Create mapping of protein IDs to matrix indices\n",
    "protein_to_idx = {protein: idx for idx, protein in enumerate(proteins)}\n",
    "\n",
    "# Create empty sparse matrix in COO format\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "# Fill the sparse matrix (only for proteins in Uniprot_A)\n",
    "for _, row in orig.iterrows():\n",
    "    protein_a = row['Uniprot_A']\n",
    "    protein_b = row['Uniprot_B']\n",
    "    \n",
    "    if protein_a not in protein_to_idx:\n",
    "        continue\n",
    "    # Only add edge if both proteins are in our protein list\n",
    "    if protein_b in protein_to_idx:\n",
    "        i = protein_to_idx[protein_a]\n",
    "        j = protein_to_idx[protein_b]\n",
    "        # Add both directions for undirected graph\n",
    "        rows.extend([i, j])\n",
    "        cols.extend([j, i])\n",
    "        data.extend([1, 1])\n",
    "\n",
    "# Convert to CSR format\n",
    "adj_matrix = sparse.csr_matrix((data, (rows, cols)), \n",
    "                             shape=(len(proteins), len(proteins)))\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Number of unique proteins from Uniprot_A: {len(proteins)}\")\n",
    "print(f\"Shape of adjacency matrix: {adj_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {adj_matrix.nnz}\")\n",
    "print(f\"Sparsity: {adj_matrix.nnz/(len(proteins)**2)*100:.2f}%\")\n",
    "\n",
    "dense_subset = adj_matrix.toarray()\n",
    "multi_label_targets = torch.tensor(dense_subset, dtype=torch.float32)\n",
    "multi_label_targets.shape\n",
    "# # Save in compressed sparse format\n",
    "# sparse.save_npz('protein_adjacency_sparse.npz', adj_matrix)\n",
    "\n",
    "# # Save the protein index mapping\n",
    "# with open('protein_index_mapping.json', 'w') as f:\n",
    "#     json.dump(protein_to_idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = multi_label_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16565, 1280]), torch.Size([16565, 16565]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"X_train\": X_train_set, \"Y_train\": Y_train_set, \"X_val\": X_val_set, \"Y_val\": Y_val_set}, \"ppi_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_841634/2198946680.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(\"ppi_dataset.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([13252, 1280])\n",
      "Y_train shape: torch.Size([13252, 16565])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load dataset\n",
    "data = torch.load(\"ppi_dataset.pt\")\n",
    "\n",
    "X_train, Y_train = data[\"X_train\"], data[\"Y_train\"]\n",
    "X_val, Y_val = data[\"X_val\"], data[\"Y_val\"]\n",
    "\n",
    "Y_train = torch.clamp(Y_train.float(), min=0.0, max=1.0)\n",
    "Y_val = torch.clamp(Y_val.float(), min=0.0, max=1.0)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: torch.Size([13252, 1280]), torch.Size([13252, 16565])\n",
      "Validation Set: torch.Size([3313, 1280]), torch.Size([3313, 16565])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split indices for training (80%) and validation (20%)\n",
    "train_idx, val_idx = train_test_split(range(X_train.shape[0]), test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Training and Validation Sets\n",
    "X_train_set = X_train[train_idx]\n",
    "Y_train_set = Y_train[train_idx][:, :]  # Keep full interaction labels (against all proteins)\n",
    "\n",
    "X_val_set = X_train[val_idx]\n",
    "Y_val_set = Y_train[val_idx][:, :]  # Keep interactions against ALL proteins\n",
    "\n",
    "print(f\"Training Set: {X_train_set.shape}, {Y_train_set.shape}\")\n",
    "print(f\"Validation Set: {X_val_set.shape}, {Y_val_set.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PPI_MultiLabel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_proteins):\n",
    "        super(PPI_MultiLabel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_proteins),  # Output size = total proteins (not just validation set)\n",
    "            # nn.Sigmoid()  # Multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, protein_embedding):\n",
    "        return self.fc(protein_embedding)  # Output shape: (batch_size, num_proteins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_841634/2962992124.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 119.3602\n",
      "Precision: 0.0210, Recall: 0.0502, F1 Score: 0.0296\n",
      "Epoch 2/50, Loss: 110.1490\n",
      "Precision: 0.0204, Recall: 0.0571, F1 Score: 0.0300\n",
      "Epoch 3/50, Loss: 108.6736\n",
      "Precision: 0.0204, Recall: 0.0555, F1 Score: 0.0299\n",
      "Epoch 4/50, Loss: 107.6066\n",
      "Precision: 0.0205, Recall: 0.0544, F1 Score: 0.0298\n",
      "Epoch 5/50, Loss: 106.1633\n",
      "Precision: 0.0207, Recall: 0.0553, F1 Score: 0.0301\n",
      "Epoch 6/50, Loss: 106.1867\n",
      "Precision: 0.0212, Recall: 0.0554, F1 Score: 0.0306\n",
      "Epoch 7/50, Loss: 105.2753\n",
      "Precision: 0.0211, Recall: 0.0542, F1 Score: 0.0304\n",
      "Epoch 8/50, Loss: 104.5715\n",
      "Precision: 0.0212, Recall: 0.0542, F1 Score: 0.0305\n",
      "Epoch 9/50, Loss: 104.0647\n",
      "Precision: 0.0216, Recall: 0.0537, F1 Score: 0.0308\n",
      "Epoch 10/50, Loss: 103.7409\n",
      "Precision: 0.0223, Recall: 0.0512, F1 Score: 0.0310\n",
      "Epoch 11/50, Loss: 103.6442\n",
      "Precision: 0.0226, Recall: 0.0505, F1 Score: 0.0312\n",
      "Epoch 12/50, Loss: 104.0662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 1280  # Match X_train embedding size\n",
    "num_proteins = Y_train.shape[1]  # Total number of proteins (train + val)\n",
    "model = PPI_MultiLabel(embedding_dim, num_proteins)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "pos_weight = (Y_train == 0).sum() / (Y_train == 1).sum() / 10  # Ratio of negatives to positives\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, device=device))\n",
    "\n",
    "# Move data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "X_train_set, Y_train_set = X_train.to(device), Y_train.to(device)\n",
    "X_val_set, Y_val_set = X_val.to(device), Y_val.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Mini-batch training\n",
    "    for i in range(0, X_train_set.shape[0], batch_size):\n",
    "        batch_X = X_train_set[i:i+batch_size]\n",
    "        batch_Y = Y_train_set[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_X)\n",
    "        loss = loss_fn(preds, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # **Validation Accuracy Check Every 5 Epochs**\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_val_set)  # Predict against all proteins\n",
    "        val_preds_binary = (val_preds > 0.5).float()\n",
    "        correct = (val_preds_binary == Y_val_set).sum().item()\n",
    "        total = Y_val_set.numel()\n",
    "        accuracy = correct / total\n",
    "\n",
    "\n",
    "    val_preds_flat = val_preds_binary.cpu().numpy().flatten()\n",
    "    Y_val_flat = Y_val_set.cpu().numpy().flatten()\n",
    "\n",
    "    precision = precision_score(Y_val_flat, val_preds_flat)\n",
    "    recall = recall_score(Y_val_flat, val_preds_flat)\n",
    "    f1 = f1_score(Y_val_flat, val_preds_flat)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
