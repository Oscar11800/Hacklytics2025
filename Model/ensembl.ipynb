{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"PPIDataset.txt\",\n",
    "    sep=\" \",  \n",
    "    skiprows=1,  \n",
    "    header=None,  \n",
    "    names=[\"protein1\", \"protein2\", \"combined_score\"],  \n",
    "    dtype={\"protein1\": str, \"protein2\": str, \"combined_score\": float}\n",
    ")\n",
    "\n",
    "# Clean the protein IDs\n",
    "df[\"protein1\"] = df[\"protein1\"].str.replace(\"9606.\", \"\", regex=False)\n",
    "df[\"protein2\"] = df[\"protein2\"].str.replace(\"9606.\", \"\", regex=False)\n",
    "\n",
    "# Get unique protein IDs from both columns\n",
    "unique_proteins = pd.concat([df[\"protein1\"], df[\"protein2\"]]).unique()\n",
    "print(f\"Total unique proteins: {len(unique_proteins)}\")\n",
    "\n",
    "# Cache for storing Ensembl to UniProt mappings\n",
    "mapping_cache = {}\n",
    "\n",
    "# Function to map a batch of Ensembl IDs to UniProt IDs\n",
    "def map_batch_to_uniprot(ensembl_ids):\n",
    "    url = \"https://rest.uniprot.org/idmapping/run\"\n",
    "    data = {\"from\": \"Ensembl_Protein\", \"to\": \"UniProtKB\", \"ids\": \",\".join(ensembl_ids)}\n",
    "    response = requests.post(url, data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        job_id = response.json().get(\"jobId\")\n",
    "        status_url = f\"https://rest.uniprot.org/idmapping/status/{job_id}\"\n",
    "        \n",
    "        # Wait for the job to complete\n",
    "        while True:\n",
    "            status_response = requests.get(status_url)\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                if status_data.get(\"results\"):\n",
    "                    results_url = f\"https://rest.uniprot.org/idmapping/uniprotkb/results/{job_id}?format=tsv\"\n",
    "                    results_response = requests.get(results_url)\n",
    "                    if results_response.status_code == 200:\n",
    "                        # Parse the TSV results\n",
    "                        results = results_response.text.strip().split(\"\\n\")\n",
    "                        header = results[0].split(\"\\t\")  # Extract header\n",
    "                        print(\"TSV Header:\", header)  # Inspect the header\n",
    "                        for line in results[1:]:  # Skip header\n",
    "                            columns = line.split(\"\\t\")\n",
    "                            # Ensure the line has the expected number of columns\n",
    "                            if len(columns) == len(header):\n",
    "                                # Extract Ensembl ID and UniProt ID\n",
    "                                ensembl_id = columns[header.index(\"From\")]  # Column with Ensembl IDs\n",
    "                                uniprot_id = columns[header.index(\"Entry\")]  # Column with UniProt IDs\n",
    "                                mapping_cache[ensembl_id] = uniprot_id\n",
    "                    break\n",
    "    return\n",
    "\n",
    "# Batch size for UniProt API (max 100,000 per request)\n",
    "BATCH_SIZE = 25  # Adjust based on API limits and performance\n",
    "\n",
    "# Map unique proteins in batches\n",
    "for i in range(0, len(unique_proteins), BATCH_SIZE):\n",
    "    batch = unique_proteins[i:i + BATCH_SIZE]\n",
    "    print(f\"Mapping batch {i // BATCH_SIZE + 1} of {len(unique_proteins) // BATCH_SIZE + 1}\")\n",
    "    map_batch_to_uniprot(batch)\n",
    "    print(len(mapping_cache))\n",
    "# Apply mappings to the dataset\n",
    "df[\"protein1_uniprot\"] = df[\"protein1\"].map(mapping_cache)\n",
    "df[\"protein2_uniprot\"] = df[\"protein2\"].map(mapping_cache)\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(\"mapped_dataset.csv\", index=False)\n",
    "\n",
    "# Display the updated dataset\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
